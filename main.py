import logging
import os
import torch
import faiss
import time
from modules.document_handler import load_hashes, save_hashes, index_documents_recursive
from modules.indexer import initialize_faiss_store
from modules.search import search_haystack_documents, search_langchain_documents
from haystack.document_stores import FAISSDocumentStore
from haystack.nodes import EmbeddingRetriever
from langchain.vectorstores import FAISS
from langchain.embeddings import SentenceTransformerEmbeddings
from langchain.schema import Document  # Document import√°l√°sa
from haystack.nodes import FARMReader

# Logging be√°ll√≠t√°sa
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# WSL-specifikus be√°ll√≠t√°sok
LIBRARY_PATH = "/mnt/c/GPT AI/Library"
SQL_URL = "sqlite:////mnt/c/GPT AI/Library index/faiss_document_store.db"
EMBEDDING_DIM = 768
BATCH_SIZE = 500
HASHES_FILE = "/mnt/c/GPT AI/Library index/document_hashes.json"
VECTORSTORE_PATH = "/mnt/c/GPT AI/LangChain/vectorstore"
FAISS_INDEX_PATH = "/mnt/c/GPT AI/Library index/faiss_index"
INDEX_FILE = "/mnt/c/GPT AI/Library index/faiss_index/index.faiss"
JSON_FILE = "/mnt/c/GPT AI/Library index/faiss_index/faiss_index.json"

# GPU ellen≈ërz√©s
device = "cuda" if torch.cuda.is_available() else "cpu"
logging.info(f"GPU haszn√°lata: {device}")

def update_langchain_index(document_store):
    """LangChain FAISS index friss√≠t√©se."""
    if not (os.path.exists(f"{VECTORSTORE_PATH}/index.faiss") and os.path.exists(f"{VECTORSTORE_PATH}/index.pkl")):
        logging.info("LangChain FAISS index nem tal√°lhat√≥. √öj index l√©trehoz√°sa...")

        documents = [
            Document(
                page_content=doc.content,
                metadata=doc.meta
            ) for doc in document_store.get_all_documents()
        ]

        embedding = SentenceTransformerEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2")
        vectorstore = FAISS.from_documents(documents, embedding)
        vectorstore.save_local(VECTORSTORE_PATH)

        logging.info("LangChain FAISS index sikeresen l√©trehozva.")
    else:
        logging.info("LangChain FAISS index m√°r l√©tezik.")

if __name__ == "__main__":
    logging.info("Haystack dokumentumt√°r inicializ√°l√°sa...")

    if os.path.exists(FAISS_INDEX_PATH):
        logging.info("üõ† FAISS index bet√∂lt√©se kor√°bbi ment√©sb≈ël...")
        document_store = FAISSDocumentStore.load(FAISS_INDEX_PATH)
    else:
        logging.info("üÜï √öj FAISS index l√©trehoz√°sa...")
        document_store = FAISSDocumentStore(
            sql_url=SQL_URL,
            faiss_index_factory_str="Flat",
            embedding_dim=EMBEDDING_DIM,
            index="document",
            validate_index_sync=False,
            return_embedding=True
        )

    retriever = EmbeddingRetriever(
        document_store=document_store,
        embedding_model="sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
        model_format="sentence_transformers",
        use_gpu=torch.cuda.is_available()
    )

    docs_count = document_store.get_document_count()
    logging.info(f"üìå FAISS index dokumentumsz√°ma: {docs_count}")

    if docs_count == 0:
        logging.warning("‚ö†Ô∏è Az FAISS index √ºres! Embeddingek √∫jragener√°l√°sa...")
        document_store.update_embeddings(retriever, batch_size=BATCH_SIZE)
        logging.info("‚úÖ Embeddingek sikeresen friss√≠tve.")

    document_hashes = load_hashes(HASHES_FILE)
    logging.info("Dokumentumok indexel√©se...")
    index_documents_recursive(LIBRARY_PATH, document_store, document_hashes, retriever, BATCH_SIZE, HASHES_FILE)
    update_langchain_index(document_store)

    logging.info("Hash f√°jl ellen≈ërz√©se...")
    if os.path.exists(HASHES_FILE):
        logging.info("Hash f√°jl sikeresen l√©trehozva.")
    else:
        logging.error("Hash f√°jl nem tal√°lhat√≥.")

    reader = FARMReader(model_name_or_path="deepset/roberta-base-squad2", use_gpu=torch.cuda.is_available())    

    query = "Magyarorsz√°g f≈ëv√°rosa?"
    results = search_haystack_documents(query, retriever, reader, top_k_retriever=10, top_k_reader=1)
    langchain_results = search_langchain_documents(query, top_k=5)

    if results:
        logging.info(f"Keres√©si eredm√©nyek a k√©rd√©sre: '{query}'")
        for result in results:
            answer = getattr(result, "answer", "Nincs v√°lasz")
            score = getattr(result, "score", 0)
            document_name = result.meta.get("name", "Ismeretlen dokumentum") if hasattr(result, "meta") else "Ismeretlen dokumentum"
            logging.info(f"- V√°lasz: {answer} | Pontoss√°g: {score:.2f} | Dokumentum: {document_name}")
    else:
        logging.info(f"Nincs tal√°lat a k√©rd√©sre: '{query}'")

# üî¥ 1. F√°jlok teljes t√∂rl√©se (ha l√©teznek)
def force_delete_faiss_files():
    index_file = os.path.join(FAISS_INDEX_PATH, "index.faiss")
    json_file = os.path.join(FAISS_INDEX_PATH, "faiss_index.json")

    for file_path in [index_file, json_file]:
        try:
            if os.path.exists(file_path):
                os.remove(file_path)
                logging.info(f"üóëÔ∏è T√∂r√∂lve: {file_path}")
                time.sleep(0.1)  # V√°runk, hogy a rendszer teljesen felszabad√≠tsa a f√°jlt
        except Exception as e:
            logging.error(f"‚ùå Hiba t√∂rt√©nt a f√°jl t√∂rl√©se k√∂zben: {e}")

force_delete_faiss_files()

# üî¥ 2. FAISS index teljes √∫jra√©p√≠t√©se
logging.info("üõ† FAISS index √∫jra√©p√≠t√©se...")

try:
    # Ha a dokumentumt√°r √ºres vagy hib√°s, hozzunk l√©tre egy teljesen √∫jat!
    if document_store.get_document_count() == 0:
        logging.warning("‚ö†Ô∏è A FAISS index √ºres, √∫j index inicializ√°l√°sa...")
        document_store = FAISSDocumentStore(
            sql_url="sqlite:///",
            faiss_index_factory_str="Flat",
            embedding_dim=768,
            index="document",
            return_embedding=True
        )
    
    # üî¥ 3. FAISS index ment√©se (v√°ltozatlanul hagyva!)
    logging.info("üõ† FAISS index ment√©se le√°ll√≠t√°s el≈ëtt...")
    document_store.save(FAISS_INDEX_PATH)
    logging.info("‚úÖ FAISS index sikeresen mentve.")

except Exception as e:
    logging.error(f"‚ùå Hiba a FAISS index ment√©se sor√°n: {e}")




